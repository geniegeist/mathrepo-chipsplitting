{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21d800cb-8438-4f9f-99cb-433279f2d214",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import pickle\n",
    "from chipsplitting import PascalForm, HyperfieldHomogeneousLinearSystem as HVLinearSystem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972da369-f19b-4a99-8914-3e0d1659c291",
   "metadata": {},
   "source": [
    "*Note: This notebook uses Cython. Compile the code by running `python setup.py build_ext --inplace` in your terminal*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60c455-b5af-4a72-aede-6f9a25504f1a",
   "metadata": {},
   "source": [
    "# Support Candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8820d046-75f0-4026-b61c-ec6d63e2347d",
   "metadata": {},
   "source": [
    "To search for fundamental statistical models in $\\Delta_n$, we consider the equivalent problem of finding fundamental chipsplitting models with a positive support size of $n+1$. A naive approach would be to enumerate all supports and, for each, check if it constitutes a fundamental chipsplitting model. This would result in $\\binom{(d+1)(d+2)/2}{n+1}$ computations.\n",
    "\n",
    "---\n",
    "\n",
    "### Optimized Search Using Remark 6.11 of [BM25]\n",
    "\n",
    "We reduce this search space using Remark 6.11 of [BM25]. The initial implementation of this algorithm was provided by Bik and Marigliano on [Mathrepo](https://mathrepo.mis.mpg.de/ChipsplittingModels/Section8.html#Section-8).\n",
    "\n",
    "We provide a Cython version of the same algorithm (see Step 1, Step 2, and Step 3 [BM25_MATHREPO]) to speed up computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25fdecc8-03f7-485a-bc8b-c492b869da5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# C++ algorithm of Bik and Marigliano \n",
    "def find_positive_supports(pos_support_size, d):\n",
    "    base_types = [\"diag\", \"row\", \"col\"]\n",
    "    A = [PascalForm(d, b, k).to_hyperfield() for b in base_types for k in range(d + 1)]\n",
    "    return HVLinearSystem(A).quick_solve_loop_fast(pos_support_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7d7752-4601-4c5c-8fd3-d6dece621845",
   "metadata": {},
   "source": [
    "### Computations $\\Delta_7$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "914ac4cc-007e-43cb-b9cc-9d03bba368c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation...\n",
      "------------------------------------------------------------\n",
      "n=7 , d=7  | Supports found: 76922   | Time:  0.19s\n",
      "n=7 , d=8  | Supports found: 436896  | Time:  1.06s\n",
      "n=7 , d=9  | Supports found: 1927201 | Time:  5.13s\n",
      "n=7 , d=10 | Supports found: 6508380 | Time: 19.66s\n",
      "n=7 , d=11 | Supports found: 18676991 | Time: 60.78s\n",
      "n=7 , d=12 | Supports found: 47682475 | Time: 163.59s\n",
      "n=7 , d=13 | Supports found: 107547676 | Time: 403.89s\n",
      "------------------------------------------------------------\n",
      "Total computation time: 654.30s\n"
     ]
    }
   ],
   "source": [
    "to_compute = [(7,7), (7,8), (7,9), (7,10), (7,11), (7,12), (7,13)]\n",
    "\n",
    "print(\"Starting computation...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_start_time = time.perf_counter()\n",
    "for n, d in to_compute:\n",
    "    iter_start_time = time.perf_counter()\n",
    "    pos_support_size = n + 1\n",
    "    possible_supports = find_positive_supports(pos_support_size, d)\n",
    "    iter_elapsed_time = time.perf_counter() - iter_start_time\n",
    "    print(f\"n={n:<2}, d={d:<2} | Supports found: {len(possible_supports):<7} | Time: {iter_elapsed_time:>5.2f}s\")\n",
    "\n",
    "total_elapsed_time = time.perf_counter() - total_start_time\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total computation time: {total_elapsed_time:.2f}s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a785802c-b39f-44b1-8708-f485d6441903",
   "metadata": {},
   "source": [
    "### Computations $\\Delta_8$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33022eaf-4ce1-4f31-85b4-1ec43b8b0fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting computation...\n",
      "------------------------------------------------------------\n",
      "n=8 , d=8  | Supports found: 622003  | Time: 11.65s\n",
      "n=8 , d=9  | Supports found: 4470103 | Time: 12.49s\n",
      "n=8 , d=10 | Supports found: 21517185 | Time: 67.20s\n",
      "n=8 , d=11 | Supports found: 88151373 | Time: 291.52s\n",
      "------------------------------------------------------------\n",
      "Total computation time: 382.86s\n"
     ]
    }
   ],
   "source": [
    "to_compute = [(8,8), (8,9), (8,10), (8,11)]\n",
    "\n",
    "print(\"Starting computation...\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "total_start_time = time.perf_counter()\n",
    "for n, d in to_compute:\n",
    "    iter_start_time = time.perf_counter()\n",
    "    pos_support_size = n + 1\n",
    "    possible_supports = find_positive_supports(pos_support_size, d)\n",
    "    iter_elapsed_time = time.perf_counter() - iter_start_time\n",
    "    print(f\"n={n:<2}, d={d:<2} | Supports found: {len(possible_supports):<7} | Time: {iter_elapsed_time:>5.2f}s\")\n",
    "\n",
    "total_elapsed_time = time.perf_counter() - total_start_time\n",
    "print(\"-\" * 60)\n",
    "print(f\"Total computation time: {total_elapsed_time:.2f}s\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950df510-bb28-4541-b8d8-8fed4d86c843",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional Optimization: Remove Non-Minimal Supports\n",
    "\n",
    "The previous step, `find_positive_supports(n+1, d)`, generated candidate positive supports $S \\subset {V_d}$ for fundamental models. For example, with $n=8, d=11$, 622,003 supports were returned.\n",
    "\n",
    "These returned supports may have a size less than $n+1$. For instance, $S = \\{(1, 1),(0, 3),(3, 0),(3, 7),(2, 9),(7, 4),(8, 3)\\}$ is a candidate support. \n",
    "\n",
    "If our target size is $n+1$, we would need to extend it by adding $(n+1 - |S|)$ many tuples $(i,j) \\in {V_d}$. This necessitates checking $\\binom{(d+1)(d+2)/2 - |S|}{n+1 - |S|}$ combinations for a given candidate $S$. To optimize, we are only interested in minimal support sets. \n",
    "\n",
    "**Optimization:** If $S' \\supset S$, then $S'$ can be excluded, as $S$ is already a more concise candidate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5255963d-435b-485e-a81e-fff46125a072",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocess\n",
    "from itertools import repeat\n",
    "\n",
    "def find_length_change_indices(sorted_data):\n",
    "    \"\"\"\n",
    "    Detects the changes in the length of the items in a sorted list of sets\n",
    "    and returns the indices that define batches of same-length items.\n",
    "\n",
    "    Args:\n",
    "        sorted_data: A list of sets, sorted by length in ascending order.\n",
    "\n",
    "    Returns:\n",
    "        A list of integers representing the starting indices of each batch\n",
    "        of same-length sets, plus the total length of the data at the end.\n",
    "    \"\"\"\n",
    "    if not sorted_data:\n",
    "        return [0]\n",
    "\n",
    "    indices = [0]\n",
    "    current_len = len(sorted_data[0])\n",
    "    for i, item in enumerate(sorted_data):\n",
    "        if len(item) != current_len:\n",
    "            indices.append(i)\n",
    "            current_len = len(item)\n",
    "\n",
    "    if indices[-1] != len(sorted_data):\n",
    "        indices.append(len(sorted_data))\n",
    "        \n",
    "    return indices\n",
    "\n",
    "# Example Usage of find_length_change_indices:\n",
    "# sorted_data = [\n",
    "#     {'a'},                  # Length 1\n",
    "#     {'b'},                  # Length 1\n",
    "#     {'c', 'd'},             # Length 2\n",
    "#     {'e', 'f'},             # Length 2\n",
    "#     {'g', 'h', 'i'},        # Length 3\n",
    "#     {'j', 'k', 'l', 'm'}    # Length 4\n",
    "# ]\n",
    "#\n",
    "# result = find_length_change_indices(sorted_data)\n",
    "# print(result)  # Expected output: [0, 2, 4, 5, 6]\n",
    "    \n",
    "def _is_minimal_in_batch(s, current_minimal_sets):\n",
    "    \"\"\"\n",
    "    A worker function for parallel processing. It checks if a single set 's'\n",
    "    is a superset of any of the already confirmed minimal sets.\n",
    "    This function is executed by each process in the pool.\n",
    "    \"\"\"\n",
    "    for m in current_minimal_sets:\n",
    "        if m.issubset(s):\n",
    "            return None  # It is a superset, therefore not minimal\n",
    "    return s  # It is minimal with respect to the sets checked\n",
    "\n",
    "def find_minimal_sets_parallel(initial_data, num_processes=None):\n",
    "    \"\"\"\n",
    "    Finds the minimal sets from a list of sets using parallel processing.\n",
    "\n",
    "    This function batches the input data by the length of the sets and\n",
    "    processes each batch of same-length sets in parallel.\n",
    "\n",
    "    Args:\n",
    "        initial_data: A list of lists, sets, or other iterables.\n",
    "        num_processes: The number of CPU processes to use.\n",
    "                       Defaults to the number of cores on the machine.\n",
    "\n",
    "    Returns:\n",
    "        A set of frozensets, where each frozenset is a minimal set.\n",
    "    \"\"\"\n",
    "    processed_data = {frozenset(item) for item in initial_data}\n",
    "    sorted_data = sorted(list(processed_data), key=len)\n",
    "\n",
    "    if not sorted_data:\n",
    "        return set()\n",
    "\n",
    "    batch_indices = find_length_change_indices(sorted_data)\n",
    "    minimal_sets = set()\n",
    "\n",
    "    with multiprocess.Pool(processes=num_processes) as pool:\n",
    "        for i in range(len(batch_indices) - 1):\n",
    "            start_index = batch_indices[i]\n",
    "            end_index = batch_indices[i+1]\n",
    "            batch = sorted_data[start_index:end_index]\n",
    "\n",
    "            # The results from the map will be either the set itself (if minimal) or None\n",
    "            results = pool.starmap(\n",
    "                _is_minimal_in_batch,\n",
    "                zip(batch, repeat(minimal_sets))\n",
    "            )\n",
    "\n",
    "            # Collect the new minimal sets found in the current batch\n",
    "            new_minimal_in_this_batch = {res for res in results if res is not None}\n",
    "            minimal_sets.update(new_minimal_in_this_batch)\n",
    "\n",
    "    return minimal_sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a27cd-b73a-4c39-a1b0-058649464a60",
   "metadata": {},
   "source": [
    "### Computations $\\Delta_7$ (Minimal support)\n",
    "\n",
    "We apply this optimization technique to $n=7, d=7,8$ to reduce the search space further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "047b798c-8440-4170-942c-8b5575b0b81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=7 , d=7  | Mininmal supports found: 15779   | Time:  5.28s\n",
      "n=7 , d=8  | Mininmal supports found: 64000   | Time: 63.35s\n"
     ]
    }
   ],
   "source": [
    "to_compute = [(7,7), (7,8)]\n",
    "for n, d in to_compute:\n",
    "    pos_support_size = n + 1\n",
    "    data = find_positive_supports(pos_support_size, d)\n",
    "    total_start_time = time.perf_counter()\n",
    "    min_data = find_minimal_sets_parallel(data)\n",
    "    iter_elapsed_time = time.perf_counter() - total_start_time\n",
    "    print(f\"n={n:<2}, d={d:<2} | Mininmal supports found: {len(min_data):<7} | Time: {iter_elapsed_time:>5.2f}s\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9ab56e-3045-47f8-af28-4bd657478834",
   "metadata": {},
   "source": [
    "### Computations $\\Delta_8$ (Minimal support)\n",
    "\n",
    "We apply this optimization technique to $n=8, d=8,9$ to reduce the search space further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2ffefb0-b24c-432f-b5e7-64090273a192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n=8 , d=8  | Mininmal supports found: 66830   | Time: 102.85s\n",
      "n=8 , d=9  | Mininmal supports found: 462075  | Time: 3950.16s\n"
     ]
    }
   ],
   "source": [
    "to_compute = [(8,8), (8,9)]\n",
    "for n, d in to_compute:\n",
    "    pos_support_size = n + 1\n",
    "    data = find_positive_supports(pos_support_size, d)\n",
    "    total_start_time = time.perf_counter()\n",
    "    min_data = find_minimal_sets_parallel(data)\n",
    "    iter_elapsed_time = time.perf_counter() - total_start_time\n",
    "    print(f\"n={n:<2}, d={d:<2} | Mininmal supports found: {len(min_data):<7} | Time: {iter_elapsed_time:>5.2f}s\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df06439-408c-44be-bad7-76f4b2107ab5",
   "metadata": {},
   "source": [
    "## Serializing candidate supports\n",
    "\n",
    "To serialize the candidate supports for later computations, include the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e558d-b467-4c7e-8401-b11ac7d3caf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/n{n:02d}_d{d:02d}.pkl', 'wb') as f:\n",
    "        pickle.dump(possible_supports, f)\n",
    "        print(f\"Serialized to {f.name}\")\n",
    "        print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e11b23-f674-47dc-8a23-ec51b4830a39",
   "metadata": {},
   "source": [
    "To deserialize the candidate supports, use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa10f56-3d87-4508-abe5-e2c95f32b885",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'data/n{n:02}_d{d:02}.pkl', 'rb') as f:\n",
    "    data = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
